# A Year in Cursor: One Billion Tokens and What I Learned

Throughout this year, I've used Cursor as my primary interface for AI engagement—for coding, technical writing, research, and product work. When Cursor released their year in review analytics, I discovered something that made me pause: I'd consumed over a billion tokens. Actually, let me be more precise: I used over a million tokens this year.

That number sits at the top of my analytics dashboard, and it raises questions I hadn't fully considered until I saw it in black and white. At my company, I'm consistently in the top 1% of users for token consumption, typically ranking in the top 10 on a weekly basis. Is that good? Does it mean I'm being productive, or wasteful? Am I providing too much context because I don't trust the AI, or am I building the right kind of context for complex work?

This article is my reflection on a year of coding and writing with Cursor. It's not a framework or a guide—it's an honest look at what the analytics reveal about how I work, what patterns emerged, and what I'm planning to change (or keep) in the year ahead.

![My Cursor Year in Review Summary](images/a_year_in_cursor_2025/year_summary.png)

## The Numbers

Let me start with what the analytics show.

### Token Consumption

![Token Consumption Summary](images/a_year_in_cursor_2025/token_consumption.png)

### Message Costs

![Message Costs Breakdown](images/a_year_in_cursor_2025/message_costs.png)

The cost per message ranges from 5 to 75 cents. When I think about this compared to mechanical turk tasks or other human-in-the-loop services, this seems like a reasonable substitute. But it also makes me pause: my tasks are being monetized at this level of granularity. Each message, each context window, each token has a cost. Is this sustainable? Is this efficient?

### Model Preferences Over Time

![Model Preferences Over Time](images/a_year_in_cursor_2025/model_preferences.png)

### Coding Patterns

![Coding Patterns Over the Year](images/a_year_in_cursor_2025/coding_patterns.png)

### Complexity and Request Types

![Complexity and Request Types](images/a_year_in_cursor_2025/complexity_patterns.png)

## The Questions I'm Asking Myself

Seeing these numbers has forced me to confront questions I'd been avoiding.

### Is It Good to Be at the Top of the Leaderboard?

Being in the top 1% of token consumers could mean several things:
- I'm experimenting aggressively and figuring things out
- I'm being wasteful with tokens
- I'm working on complex problems that require extensive context
- I'm not optimizing my prompts or context windows

The truth is probably some combination of all of these. But the question isn't just whether I'm using a lot of tokens—it's whether that usage is translating into value.

### Does High Token Usage Equate to AI Adoption?

High token consumption might reflect AI adoption, but it doesn't necessarily reflect effective adoption. Am I using Cursor because it's the right tool for the job, or am I using it because it's become a habit? Is my usage pattern showing that I'm learning to work with AI more effectively, or am I just throwing more tokens at problems?

### Am I Being More Productive, or Just Wasteful?

This is the question that keeps me up at night. I feel more productive. I'm shipping more content, writing more frameworks, building more prototypes. But is that because Cursor is making me more efficient, or am I just doing more work in the same amount of time? Without clear productivity metrics, it's hard to know.

### Am I Providing Too Much Context?

I tend to provide a lot of context in my prompts. Is this good practice—ensuring the AI has everything it needs to understand the problem—or does it show I still have low trust in the AI's ability to work with less? 

Looking at my patterns, I see I'm often including entire files, multiple reference documents, and extensive background. This might be necessary for complex technical writing and framework development, but it also drives up token costs. Should I be more surgical with context? Would tighter rules and better specifications reduce my context window needs?

### Task Distribution and Complexity

I'm pretty happy with the distribution of tasks I'm working on. The analytics show a good mix: documentation, technical writing, code refactoring, new features, configuration work. But I don't have clear task mapping to task complexity. 

How do I break into high-complexity tasks? Am I avoiding them, or am I just not recognizing when a task is high-complexity? The analytics show my complexity distribution, but I need to understand what's driving it.

### Product Person First, Coder Second

I use Cursor a lot for context building, documentation, and technical writing. I'm a product person first and a coder second. Not to say I don't write code, but I spend a lot of time communicating ideas to a broader audience, and not everyone writes code. So the "documentation" KLO classification makes sense.

But this raises another question: am I using Cursor effectively for the work I actually do? The tool is optimized for coding, but I'm using it heavily for writing and communication. Is that the right fit?

### Query Efficiency and Spec-Driven Design

Are my queries efficient? Does the pattern show I'm using spec-driven design more often, or am I still in "yolo vibe coding" mode? 

Looking at my message patterns, I see a lot of iterative refinement. I'll send a message, get a response, refine, send another message. This might be spec-driven (defining what I want, then iterating), or it might be exploratory (trying things until something works). The analytics don't clearly distinguish between these patterns.

### Rule Size and Context Windows

Do I need smaller rules? Maybe I'm dumping too much information into context, and if my rules were tighter, my context windows would be smaller and I'd use fewer tokens?

This is a technical question with real cost implications. If I can achieve the same results with smaller context windows, I should. But I also don't want to sacrifice quality for efficiency. Finding that balance is the challenge.

## What I've Learned

After a year of heavy Cursor usage, here's what I've learned:

### Cursor Is My Primary AI Interface

I plan to continue using Cursor as my primary interface for AI engagement. This doesn't mean I won't use other AI experiences, but this is where I start. Everything in a repo. All context for analysis referable in markdown files. All incremental deliverables actualized in markdown files and highly organized in directories. Every task is going to be automated and repeatable when it comes to research, technical writing, product work, and code.

### I'm the Bottleneck

I'm the bottleneck here. I need to get faster at this process and flow. I think this is the future, and I'm expecting that many of my projects will start to show ROI from the foundation I've established with AI-accessible resources in the new year.

The tools are there. The patterns are emerging. The question is whether I can move fast enough to capitalize on them.

### This Is an Opportunity to Reflect and Experiment

This reflection isn't just about looking back—it's about determining what new experiments, pilots, and trials I want to run in the fiscal year ahead. What patterns should I double down on? What should I change? How do I measure whether I'm improving?

## My Goals for the Year Ahead

Based on this reflection, here are my top three goals and how I'll measure them:

### Goal 1: [TBD - Optimize Context Windows]

**What I'll do:** [Description of the experiment or change]

**How I'll measure it:**
- [Metric 1]
- [Metric 2]
- [Metric 3]

### Goal 2: [TBD - Increase High-Complexity Task Engagement]

**What I'll do:** [Description of the experiment or change]

**How I'll measure it:**
- [Metric 1]
- [Metric 2]
- [Metric 3]

### Goal 3: [TBD - Improve Query Efficiency]

**What I'll do:** [Description of the experiment or change]

**How I'll measure it:**
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Conclusion

A year of heavy Cursor usage has taught me as much about my own work patterns as it has about the tool itself. The analytics reveal habits I didn't know I had, patterns I wasn't aware of, and questions I need to answer.

I'm not stopping. I'm doubling down. But I'm also being more intentional about how I use the tool, what I measure, and what I optimize for.

The future of technical writing, product work, and code development is AI-assisted. I've built a foundation this year. Now it's time to see what I can build on top of it.

## References

- [Cursor Pricing Documentation](https://cursor.com/docs/account/pricing)
- [Tokens & Pricing Guide](https://cursor.com/learn/tokens-pricing)
- [Cursor Analytics Documentation](https://cursor.com/docs/account/teams/analytics)
